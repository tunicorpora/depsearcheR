---
title: "depsearcheR -- basic usage"
author: "Juho HÃ¤rme"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{depsearcheR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


The DepsearcheR package is a simple utility made for the purpose of using R for
corpus analyses involving the utilization of dependency annotations represented
in some version of the CoNLL format (cf. e.g [here](https://stackoverflow.com/questions/27416164/what-is-conll-data-format)).

## Example data
 

Assume that you have parsed a file like the one provided in the `inst/extdata`
folder of this package (the Finnish wikipedia article for sparrow):

```{r}

library(depsearcheR)
library(readr)
mytext <- readr::read_file(
                           system.file("extdata", 
                                       "varpunen_wikipedia.txt",
                                       package="depsearcheR")
                           )

cat(substr(mytext,1,300))


```

The text has been parsed with [the Finnish dependency parser developed at the
university of Turku](http://turkunlp.github.io/Finnish-dep-parser/) and this
output file is also included in `inst/extdata`. Note that 
the format here is the so called universal dependencies format.
This is what the conll formatted file looks like:


    1	Varpunen	varpunen	NOUN	_	Case=Nom|Number=Sing	8	nsubj:cop	_	_
    2	(	(	PUNCT	_	_	4	punct	_	_
    3	Passer	Passer	PROPN	_	Case=Nom|Number=Sing	4	compound:nn	_	_
    4	domesticus	domesticus	NOUN	_	Case=Nom|Number=Sing	1	appos	_	_
    5	)	)	PUNCT	_	_	4	punct	_	_
    6	on	olla	VERB	_	Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act	8	cop	_	_
    7	yleinen	yleinen	ADJ	_	Case=Nom|Degree=Pos|Number=Sing	8	amod	_	_
    8	lintulaji	lintu#laji	NOUN	_	Case=Nom|Number=Sing	0	root	_	_
    9	suuressa	suuri	ADJ	_	Case=Ine|Degree=Pos|Number=Sing	10	amod	_	_
    10	osassa	osa	NOUN	_	Case=Ine|Number=Sing	8	nmod	_	_
    11	Eurooppaa	Eurooppa	PROPN	_	Case=Par|Number=Sing	10	nmod	_	_
    12	ja	ja	CONJ	_	_	11	cc	_	_
    13	Aasiaa	Aasia	PROPN	_	Case=Par|Number=Sing	11	conj	_	_
    14	.	.	PUNCT	_	_	8	punct	_	_

    1	Carolus	Carolus	PROPN	_	Case=Nom|Number=Sing	2	name	_	_
    2	Linnaeus	Linnaeus	PROPN	_	Case=Nom|Number=Sing	3	nsubj	_	_
    3	antoi	antaa	VERB	_	Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin|Voice=Act	0	root	_	_
    4	varpuselle	varpunen	NOUN	_	Case=All|Number=Sing	3	nmod	_	_
    5	aluksi	aluksi	ADV	_	_	3	advmod	_	_
    6	nimen	nimi	NOUN	_	Case=Gen|Number=Sing	7	nmod:poss	_	_
    7	Fringilla	Fringilla	NOUN	_	Case=Ade|Number=Plur	3	nmod	_	_
    8	domestica	domestica	X	_	Foreign=Foreign	3	dobj	_	_
    9	.	.	PUNCT	_	_	3	punct	_	_


Now, let's imagine we have a data set consisting of all the sentences of
the Finnish wikipedia article mentioned above. It could be acquired
as follows:

```{r}


library(dplyr)
library(readr)
sentences <- readr::read_file(
                           system.file("extdata", 
                                       "varpunen.conll",
                                       package="depsearcheR")
                           ) %>% 
        strsplit("\n\n")  %>% 
        unlist



```


## Usage on the sentence level

At the heart of the depsearcheR package is an extremely simple
function called `FilterConllRows`. The idea of this function
is to filter a conll formatted sentence according to some
conditions given by the users. The output of the filter
can then be used for more filtering.


As simple and trivial scenario, imagine that we want to retrieve all the
nouns of a sentence. This could be achieved in the following way
(using the example set of sentences from the previous section):

```{r}

FilterConllRows(sentences[1], "pos", "NOUN")

```

The function can also be used with regular expressions:


```{r}
FilterConllRows(sentences[1], "feat", "Case=Ine", use_regex=T)
```

...conditions with multiple values

```{r}
FilterConllRows(sentences[1], "pos", c("NOUN","ADJ"))
```

or with negative conditions, for intance to get everything
that's not a noun or an adjective:

```{r}
FilterConllRows(sentences[1], "pos", c("NOUN","ADJ"), is_negative=T)
```

You can add more complex conditions by piping the 
reslts in `dplyr` style. For instance, to 
get something that is not a noun but is in the inessive
case:

```{r}
FilterConllRows(sentences[1], "pos", c("NOUN","PROPN")) %>% 
    FilterConllRows("feat", "Case=Ine", use_regex=T) 
```

This is where the function actually gets useful for
getting information about dependencies.


Now, imagine we want to get all the dependents of a 
finite verb in a sentence. For that, we can use
the values of the columns `head` and `tokenid`.

```{r}


#Get all the finite verbs in the sentence
finverbs <- FilterConllRows(sentences[2], "feat", "VerbForm=Fin", T)
#Get their dependents
deps <- FilterConllRows(sentences[2], "head", finverbs$tokenid)
deps
                         
```

There is actually a shortcut function for this (`GetDeps`). It takes
exactly the same arguments as 'FilterConllRows'.
Hence, the previous example could also be written as:


```{r}


#Get the dependents of finite verbs
deps <- GetDeps(sentences[2],"feat","VerbForm=Fin", T)
deps
                         
```

There's also a convienience function for testing
if a sentence contains a certain dependency relation,
say, if there is at least one adjective governed
by a noun. This function is called `ContainsDepRel`
and it can be used as follows:

```{r}

ContainsDepRel(sentences[1],
             depw=c("pos","ADJ"),
             headw=c("pos","NOUN")
             )


```



## Usage on the dataset level

All the previous examples demonstrate the usage of the simple functions in
this package on the sentence level. What the whole package is actually
for, is, however, querying datastructures containing multiple sentences
and filtering only the ones that are relevant for the user.

It should be emphasized at this point that this package is definitely not
meant for effective data mining on a large scale. The 
functions used are simple, perhaps even trivial, and performance
is the cost. But if you know what you're searching for and 
are not dealing with a terribly large dataset (or have a lot of time)
this can be a useful approach.

### Querying a raw text

The first, although propably not the most common, use case considered
here is where you have a conll formatted text (such as the one from the first section
of this vignette) and you want to make queries to find sentences with certain
characteristics. To do that, we want to have the data formatted
as a vector of sentences.

Since the data in conll format separates sentences with an empty row, 
a vector of sentences can be obtained simply by splitting the file
by to consequtive newlines as was done at the beginning of this
vignette. There is also a convenience function for this, which
takes a filename as a parameter and produces the vector:

```{r}

sentences  <- GetSentencesFromFile("../inst/extdata/varpunen.conll")

```

Now, let's say we want to get all the sentences 

1) which include a finite verb
2) in which the finite verb has a subject dependent
3) in which the subject is a  proper name

<!-- -->

The trick here is to develop custom functions that will take care 
of the filtering and then apply those functions to the vector
containing the sentences. Here's an example function that can be used
for searching for only the kind of results described above (has a finite verb,
the verb has a subject dependent, the subject is a proper name)

```{r}

MyFilterFunction <- function(sentence) {
            my_matches <- 
                GetDeps(sentence,"feat","VerbForm=Fin", T) %>% 
                FilterConllRows("dep","nsubj") %>% 
                FilterConllRows("pos","PROPN") 

            return(nrow(my_matches))
} 


```

There is one requirement to keep in mind when creating 
these filters: every filter function should return
a truthy value based on the matches retrieved. The simplest
way to do this is to return the number of rows in the
tibble returned by the sentence filters.
To run the filter, we can use R's built-in `Filter` function
(note: uppercase F), which takes a function as the first
argument and a vector as the second:

```{r}

matched_sentences <- Filter(MyFilterFunction, sentences)

```

In our case, there was one sentence that matched the query.
To view the sentence as a tibble, we can use the function
`ConllAsTibble`:

```{r}

ConllAsTibble(matched_sentences)


```

Or, if we would like a more human-readable representation of the actual 
sentence, there is a function called `ConllAsSentence`:

```{r}

ConllAsSentence(matched_sentences)

```

Keep in mind, however, that these two functions take *single* sentences
as arguments, not vectors of sentences.

This kind of querying works okay with the kind of toy files used here
as examples. However, if we have a larger text, this really not
an effective way of mining it. However, if you don't mind
the query running for a while, you can, of course, give 
this approach a go. In that case, it might be useful to 
add a progress bar to the function, for instance 
with the `progress_estimated` function from `dplyr`.
Here's an example with the previous function
transformed to an unnamed function inside the call
to `Filter`:

```{r}
library(dplyr)

#initialize the progress bar
p <- progress_estimated(length(sentences))

matched_sentences <- Filter(
            function(sentence) {
                my_matches <- 
                    GetDeps(sentence,"feat","VerbForm=Fin", T) %>% 
                    FilterConllRows("dep","nsubj") %>% 
                    FilterConllRows("pos","PROPN") 
                #Move the progress bar:
                p$tick()$print()
                return(nrow(my_matches))
            }, sentences)

#Remove the progress bar
p$stop()

```






### Filtering concordances

The basic idea is to have the data in a data frame / tibble where one 
row corresponds to one sentence and there is a
column containing the conll representation of the sentence. In addition, you
pro




Another basic use case is if you have a data set consisting of concordances,
acquired through, for instance, a query to a web corpus




## TODO

TODO: first make a reference data frame with all data as tabbed?
TODO: function to list possible columns and values






